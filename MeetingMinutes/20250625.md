- 인우
    - Fast API가 필요없는지, Fast API에 Fast MCP 넣는 부분 확인
        
        ### FAST-MCP
        
        git clone
        
        uv init
        
        uv venv && source .venv/bin/activate
        
        uv pip install pyproject.toml
        
        uv sync
        
        .env 만들고 키 추가
        
        uv run [server.py](http://server.py/)
        
        npx @modelcontextprotocol/inspector
        
        ![image.png](attachment:1303feeb-4216-484a-b8fc-1d4a126de70c:image.png)
        
        돌아가는 거 다 끈 다음에
        
        uv run fastapi_example/server.py
        
        npx @modelcontextprotocol/inspector
        
        uvicorn은 fast api를 쓰는 애
        
        app.mount가 controller처럼 보내주는 애
        
        ![image.png](attachment:d65dbeda-5184-4b2f-88e3-16423097bbc1:image.png)
        
        ![image.png](attachment:847f98b7-27b4-4747-ae2a-f7156e99bdc8:image.png)
        
        localhost: 8000까지가 fastapi
        
        뒤가 mcp
        
        fastapi 자체가 필요한것인지에 대해서는 더 알아봐야함
        
- 정윤, 지혜
    - backend api merge되면 backend, docs, data pipeline pull 받기
    - 인우님이 만든 모델에 MCP 붙여보기
        - MCP Client에서 MCP Server와의 연결은 이상이 없으나 LLM 부분에서 문제가 있어 해결 필요
        
        server.py
        
        ```bash
        from mcp.server.fastmcp import FastMCP
        from langchain.llms.base import LLM
        from typing import Optional, List, Mapping, Any
        import requests
        import sys
        import json
        
        # Ollama LLM을 LangChain의 LLM 인터페이스에 맞게 구현한 클래스
        class OllamaLLM(LLM):
            model: str = "llama3.2:latest"
            base_url: str = "http://192.168.2.6:11434"
        
            # LangChain에서 LLM 호출 시 사용하는 내부 메서드
            def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
                print(f"[MCP 서버] OllamaLLM _call 호출됨, prompt: {prompt}", flush=True)
                url = f"{self.base_url}/api/generate"  # Ollama 요청 엔드포인트
                payload = {
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False
                }
                try:
                    # POST 요청을 통해 모델 응답 받기
                    response = requests.post(url, json=payload)
                    response.raise_for_status()
                    result = response.json()
                    return result.get("response", "[응답 없음]")
                except Exception as e:
                    # 오류 발생 시 로그 출력 및 메시지 반환
                    print("[OllamaLLM 오류]", e, flush=True)
                    return f"[오류 발생]: {e}"
        
            # LangChain에서 사용할 LLM 타입 이름 정의
            @property
            def _llm_type(self) -> str:
                return "ollama"
        
            # 모델 식별을 위한 파라미터 반환
            def _identifying_params(self) -> Mapping[str, Any]:
                return {"model": self.model}
        
        # MCP 서버 인스턴스 생성 및 커스텀 LLM 설정
        mcp = FastMCP("Ollama MCP")
        mcp.llm = OllamaLLM()
        
        # MCP 도구 정의: 기본 LLM 호출 도구
        @mcp.tool(name="llm", description="기본 LLM 호출 도구")
        def call_llm(input: str) -> str:
            return mcp.llm._call(input)
        
        # MCP 도구 정의
        @mcp.tool()
        def add(a: int, b: int) -> int:
            print("[MCP 서버] add 함수 호출됨", flush=True)
            return a + b
        
        @mcp.tool()
        def multiply(a: int, b: int) -> int:
            return a * b
        
        if __name__ == "__main__":
            print("[MCP 서버] stdio 모드 실행 중...", flush=True)
        
            # stdin에서 요청을 받아 처리하는 루프
            for line in sys.stdin:
                line = line.strip()
                if not line:
                    continue
                try:
                    req = json.loads(line)
                    args = req.get("args", {})
                    # 입력 파라미터에 따라 도구 호출 분기 처리
                    if "input" in args:
                        result = call_llm(args["input"])  # LLM 호출
                    elif "a" in args and "b" in args:
                        result = add(args["a"], args["b"])  # 덧셈 호출
                    else:
                        result = "[❗잘못된 요청]"
                    resp = {"result": result}
                except Exception as e:
                    # 예외 발생 시 오류 메시지 반환
                    resp = {"result": f"[❌ 서버 오류]: {e}"}
                # 결과를 JSON 형태로 stdout에 출력
                print(json.dumps(resp), flush=True)
        
            # stdio 기반 MCP 서버 실행 (위 루프 이후에 추가 실행됨)
            print("[MCP 서버] stdio 모드 실행 중...", flush=True)
            mcp.run(transport="stdio")
        
        ```
        
        client.py
        
        ```bash
        import subprocess
        import json
        
        # MCP 서버의 도구(tool)를 호출하는 함수
        def call_mcp_tool(prompt: str):
            # MCP 서버를 서브프로세스로 실행 (mcp_server2.py를 실행)
            process = subprocess.Popen(
                ["python", "mcp_server2.py"],           
                stdin=subprocess.PIPE,           
                stdout=subprocess.PIPE,             
                stderr=subprocess.PIPE,            
                encoding='utf-8',                     
                bufsize=1               
            )
        
            # MCP 툴에 보낼 요청 형식 정의
            mcp_request = {
                "args": {
                    "input": prompt    
                }
            }
        
            # JSON 문자열로 직렬화 후 개행 문자 추가
            json_str = json.dumps(mcp_request) + "\n"
            print("[DEBUG] 요청 JSON:", json_str.strip())
        
            # MCP 서버에 요청 전송
            process.stdin.write(json_str)
            process.stdin.flush()
        
            # MCP 서버 응답 수신 대기 루프
            while True:
                response_line = process.stdout.readline()  # 한 줄씩 읽기
                if not response_line:
                    break
                response_line = response_line.strip()
                print("[DEBUG] 응답 원본:", response_line)
                try:
                    # JSON 응답 파싱
                    response = json.loads(response_line)
                    print("[✅ 응답]:", response.get("result"))
                    break
                except Exception:
                    continue  # JSON 형식이 아니면 무시하고 계속 읽기
        
            # 서브프로세스 종료
            process.terminate()
        
        # 테스트 실행
        if __name__ == "__main__":
            call_mcp_tool("1+1=?")
        
        ```
        
- 데이터
    - 스트레칭은 난이도 초급으로 넣기
    - 개요, 효과 없는 데이터 삭제
    - 증복 제거